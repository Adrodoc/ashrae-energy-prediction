{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Tensorflow Model w/o Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notes:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_df = pd.read_csv(\"../data/raw/building_metadata.csv\")\n",
    "weather_train = pd.read_csv(\"../data/raw/weather_train.csv\")\n",
    "train = pd.read_csv(\"../data/raw/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the single dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(building_df, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\n",
    "train = train.merge(weather_train, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"])\n",
    "del weather_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce function for reducing in-memory size of data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1036.44 Mb (60.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which columns are in the set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_id', 'meter', 'timestamp', 'meter_reading', 'site_id',\n",
       "       'primary_use', 'square_feet', 'year_built', 'floor_count',\n",
       "       'air_temperature', 'cloud_coverage', 'dew_temperature',\n",
       "       'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction',\n",
       "       'wind_speed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate timestamp elements\n",
    "\n",
    "*Note: I didn't add year as single feature as it is not recurrent and not useful in future implementations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\n",
    "train[\"hour\"] = train[\"timestamp\"].dt.hour\n",
    "train[\"weekday\"] = train[\"timestamp\"].dt.weekday\n",
    "train[\"month\"] = train[\"timestamp\"].dt.month\n",
    "\n",
    "del train[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode categorical data\n",
    "\n",
    "*Note: nominal features should probably be one-hot-encoded. I got out of memory errors on the first try so maybe we are looking for better code/approach*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_cols = [\"building_id\", \"meter\", \"site_id\", \"primary_use\"]\n",
    "ordinal_cols = [\"hour\", \"weekday\", \"month\"]\n",
    "\n",
    "#one_hot = OneHotEncoder(categorical_features=nominal_cols)\n",
    "#train = one_hot.fit_transform(train)\n",
    "train[ordinal_cols + nominal_cols] = train[ordinal_cols + nominal_cols].apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Substitute NaNs with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.fillna(0)\n",
    "train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[\"meter_reading\"]\n",
    "del train[\"meter_reading\"]\n",
    "train = StandardScaler().fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.86975219, -0.71071682, -1.56469446, ..., -1.66259743,\n",
       "         0.49707282, -1.62524441],\n",
       "       [-1.86740682, -0.71071682, -1.56469446, ..., -1.66259743,\n",
       "         0.49707282, -1.62524441],\n",
       "       [-1.86506146, -0.71071682, -1.56469446, ..., -1.66259743,\n",
       "         0.49707282, -1.62524441],\n",
       "       ...,\n",
       "       [ 1.52164554, -0.71071682,  1.38097395, ...,  1.65927177,\n",
       "         0.99787434,  1.57642821],\n",
       "       [ 1.5239909 , -0.71071682,  1.38097395, ...,  1.65927177,\n",
       "         0.99787434,  1.57642821],\n",
       "       [ 1.52633627, -0.71071682,  1.38097395, ...,  1.65927177,\n",
       "         0.99787434,  1.57642821]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, y_train, test_size=0.2, random_state=420)\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"mean_squared_logarithmic_error\",\n",
    "              metrics=[\"mean_squared_logarithmic_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16100484 samples\n",
      "Epoch 1/10\n",
      "16100484/16100484 [==============================] - 64s 4us/sample - loss: 2.3559 - mean_squared_logarithmic_error: 2.3559\n",
      "Epoch 2/10\n",
      "16100484/16100484 [==============================] - 63s 4us/sample - loss: 2.0172 - mean_squared_logarithmic_error: 2.0172\n",
      "Epoch 3/10\n",
      "16100484/16100484 [==============================] - 65s 4us/sample - loss: 1.9218 - mean_squared_logarithmic_error: 1.9218\n",
      "Epoch 4/10\n",
      "16100484/16100484 [==============================] - 64s 4us/sample - loss: 1.8548 - mean_squared_logarithmic_error: 1.8548\n",
      "Epoch 5/10\n",
      "16100484/16100484 [==============================] - 64s 4us/sample - loss: 1.8189 - mean_squared_logarithmic_error: 1.8189\n",
      "Epoch 6/10\n",
      "16100484/16100484 [==============================] - 64s 4us/sample - loss: 1.7898 - mean_squared_logarithmic_error: 1.7898\n",
      "Epoch 7/10\n",
      "16100484/16100484 [==============================] - 64s 4us/sample - loss: 1.7646 - mean_squared_logarithmic_error: 1.7646\n",
      "Epoch 8/10\n",
      "16100484/16100484 [==============================] - 65s 4us/sample - loss: 1.7425 - mean_squared_logarithmic_error: 1.7425\n",
      "Epoch 9/10\n",
      "16100484/16100484 [==============================] - 65s 4us/sample - loss: 1.7233 - mean_squared_logarithmic_error: 1.7233\n",
      "Epoch 10/10\n",
      "16100484/16100484 [==============================] - 65s 4us/sample - loss: 1.7064 - mean_squared_logarithmic_error: 1.7064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f44b4bc3f90>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train.values, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7194584851329462, 1.7194499]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
